{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNP+1X8jp4/hCcJSS9yXNUv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archyyu/encoder-related/blob/main/encoder_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "I think this is a very simple Masked Language Model"
      ],
      "metadata": {
        "id": "jsAEiab8wkk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nFaVGmvM2FOC"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/archyyu/publicResource/main/chat_dataset.csv')"
      ],
      "metadata": {
        "id": "9fz7e2OpQvQt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "hidden_size = 100\n",
        "embedding_dim = 80\n",
        "seq_length = 25\n",
        "learning_rate = 1e-1\n",
        "batch_size = 20\n",
        "dropout = 0.1\n",
        "eval_iters = 200\n",
        "num_heads = 4\n",
        "head_size = 20"
      ],
      "metadata": {
        "id": "7_VDGwea6Ktm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pad = '[pad]'\n",
        "mask = '[MASK]'\n",
        "data = []\n",
        "targets = []\n",
        "for index, row in df.iterrows():\n",
        "  data.append(row['message'])\n",
        "  targets.append(row['sentiment'])\n",
        "\n",
        "datalen = []\n",
        "for line in data:\n",
        "  datalen.append(len(line.split(' ')))\n",
        "\n",
        "\n",
        "targetset = sorted(set(targets))\n",
        "sentiment_to_index = {s:i for i, s in enumerate(targetset)}\n",
        "index_to_sentiment = {i:s for i, s in enumerate(targetset)}"
      ],
      "metadata": {
        "id": "4INzfntYQ5IE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sorted(set((' '.join(data)).split(' ')))\n",
        "dataset.append(pad)\n",
        "dataset.append(mask)\n",
        "vocab_size = len(dataset)\n",
        "word_to_index = {w:i for i, w in enumerate(dataset)}\n",
        "index_to_word = {i:w for i, w in enumerate(dataset)}\n",
        "\n",
        "pad_index = word_to_index[pad]\n",
        "\n",
        "n = (int)(len(data) * 0.9)\n",
        "training_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "fYvtMMv0SXvp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines = []\n",
        "for item in data:\n",
        "  lines.append(item.split(' '))\n",
        "\n",
        "max_line = max([len(line) for line in lines])\n",
        "\n",
        "for item in lines:\n",
        "  for _ in range(max_line - len(item)):\n",
        "    item.append(pad)\n",
        "\n",
        "X = []\n",
        "for line in lines:\n",
        "  item = [word_to_index[word] for word in line]\n",
        "  X.append(item)\n",
        "\n",
        "Y = []\n",
        "for i in range(len(targets)):\n",
        "  item = sentiment_to_index[targets[i]]\n",
        "  Y.append(item)"
      ],
      "metadata": {
        "id": "nvcrv9J3Yfpt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, embedding_size, head_size):\n",
        "    super(AttentionHead, self).__init__()\n",
        "    self.head_size = head_size\n",
        "    self.C = embedding_size\n",
        "\n",
        "    self.q = nn.Linear(self.C, head_size, bias=False)\n",
        "    self.v = nn.Linear(self.C, head_size, bias=False)\n",
        "    self.k = nn.Linear(self.C, head_size, bias=False)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    B,T,C = x.shape\n",
        "    q = self.q(x)\n",
        "    k = self.k(x)\n",
        "    v = self.v(x)\n",
        "\n",
        "    wei = q @ k.transpose(-2, -1) * (self.head_size ** -0.5)\n",
        "    wei.masked_fill_(mask==0, -1e9)\n",
        "    wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "    return wei @ v\n",
        "\n",
        "class EncoderMultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, embedding_size, head_size):\n",
        "    super(EncoderMultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.heads = nn.ModuleList([\n",
        "        AttentionHead(embedding_size, head_size) for _ in range(num_heads)\n",
        "    ])\n",
        "\n",
        "    self.final_linear = nn.Linear(num_heads * head_size, embedding_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    head_outputs = [head(x, mask) for head in self.heads]\n",
        "    concatenated_output = torch.cat(head_outputs, dim=-1)\n",
        "    final_output = self.final_linear(concatenated_output)\n",
        "    return self.dropout(final_output)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, embedding_size):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(embedding_size, 4 * embedding_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(4 * embedding_size, embedding_size),\n",
        "        nn.Dropout(dropout),\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.net(x)\n",
        "\n",
        "class AddAndNormLayer(nn.Module):\n",
        "  def __init__(self, embedding_size):\n",
        "    super(AddAndNormLayer, self).__init__()\n",
        "    self.norm = nn.LayerNorm(embedding_size)\n",
        "\n",
        "  def forward(self, x, subLayer: nn.Module, mask=None):\n",
        "    if mask == None:\n",
        "      return x + subLayer(self.norm(x))\n",
        "    else:\n",
        "      return x + subLayer(self.norm(x), mask)\n",
        "\n",
        "class EncoderBlockAttention(nn.Module):\n",
        "  def __init__(self, num_heads, embedding_size, head_size):\n",
        "    super(EncoderBlockAttention, self).__init__()\n",
        "    self.multiheads = EncoderMultiHeadAttention(num_heads, embedding_size, head_size)\n",
        "    self.fw = FeedForward(embedding_size)\n",
        "    self.addLayers = nn.ModuleList([AddAndNormLayer(embedding_size) for i in range(2)])\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    inter_result = self.addLayers[0](x, self.multiheads, mask)\n",
        "    final_result = self.addLayers[1](inter_result, self.fw)\n",
        "    return final_result\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, num_heads, vocab_size, embedding_size, head_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.em = nn.Embedding(vocab_size, embedding_size)\n",
        "    self.embedding_size = embedding_size\n",
        "    self.blocks = nn.ModuleList([EncoderBlockAttention(num_heads, embedding_size, head_size) for _ in range(4)])\n",
        "    self.f_norm = nn.LayerNorm(embedding_size)\n",
        "    self.fw = nn.Linear(embedding_size, vocab_size, bias=False)\n",
        "\n",
        "  def p_en(self, x, embedding_size):\n",
        "    B,T = x.shape\n",
        "    C = embedding_size\n",
        "    n = 10000\n",
        "    x = torch.zeros((T, C))\n",
        "    for k in range(T):\n",
        "      for i in torch.arange(int(C/2)):\n",
        "        denominator = torch.pow(n, 2*i/C)\n",
        "        x[k, 2*i] = torch.sin(k/denominator)\n",
        "        x[k, 2*i+1] = torch.cos(k/denominator)\n",
        "    return x\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    B,T = x.shape\n",
        "    x_em = self.em(x)\n",
        "    p_em = self.p_en(x, self.embedding_size)\n",
        "\n",
        "    x = x_em + p_em\n",
        "    for block in self.blocks:\n",
        "      x = block(x, mask)\n",
        "    x = self.f_norm(x)\n",
        "    x = self.fw(x)\n",
        "    return x\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = Encoder(num_heads, vocab_size, embedding_dim, head_size)\n",
        "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "SIGRVqLNVdgF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch():\n",
        "  inputs = []\n",
        "  targets = []\n",
        "  mask_index_list = []\n",
        "  masks = []\n",
        "  n = torch.randint(len(X) - batch_size, [1]).item()\n",
        "  for i in range(batch_size):\n",
        "    inputs_item = torch.tensor(X[n + i])\n",
        "\n",
        "    mask_index = torch.randint(datalen[n + 1], [1])\n",
        "\n",
        "    targets.append(inputs_item[mask_index])\n",
        "    mask_index_list.append(torch.tensor([i,mask_index]))\n",
        "\n",
        "    inputs_item[mask_index] = word_to_index[mask]\n",
        "\n",
        "    inputs.append(inputs_item)\n",
        "\n",
        "    masks.append(inputs_item != pad_index)\n",
        "\n",
        "  return torch.stack(inputs), torch.stack(targets), torch.stack(mask_index_list), torch.stack(masks).unsqueeze(1)"
      ],
      "metadata": {
        "id": "h5OFKwKM6ElH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_iters = 10000\n",
        "for i in range(n_iters):\n",
        "  inputs, targets, indices, masks = get_batch()\n",
        "\n",
        "  predicts = model(inputs, masks)\n",
        "\n",
        "  #print(targets.view(-1).shape)\n",
        "  outputs = predicts[indices[:,0],indices[:,1]]\n",
        "  #print(outputs.shape)\n",
        "\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "  loss = criterion(outputs, targets.view(-1))\n",
        "\n",
        "  loss.backward()\n",
        "\n",
        "  optimizer.step()\n",
        "\n",
        "  if i%500 == 0:\n",
        "    print(f'i {i}, loss:{loss.item()}')"
      ],
      "metadata": {
        "id": "jamQ4eJv7Vdk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b9ae08d-0cc0-436e-beb1-39abf583df53"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i 0, loss:7.051021575927734\n",
            "i 500, loss:3.3548896312713623\n",
            "i 1000, loss:1.8986141681671143\n",
            "i 1500, loss:2.221907138824463\n",
            "i 2000, loss:1.1237813234329224\n",
            "i 2500, loss:1.2446720600128174\n",
            "i 3000, loss:2.3592095375061035\n",
            "i 3500, loss:1.0986623764038086\n",
            "i 4000, loss:2.5190157890319824\n",
            "i 4500, loss:0.896750807762146\n",
            "i 5000, loss:3.631227970123291\n",
            "i 5500, loss:2.441333055496216\n",
            "i 6000, loss:0.6416040658950806\n",
            "i 6500, loss:0.8256516456604004\n",
            "i 7000, loss:4.587604999542236\n",
            "i 7500, loss:0.5165543556213379\n",
            "i 8000, loss:0.9975517988204956\n",
            "i 8500, loss:0.9209945797920227\n",
            "i 9000, loss:0.3710198998451233\n",
            "i 9500, loss:1.0797696113586426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = torch.randint(len(X) - 20,[1]).item()\n",
        "for line in data[-n:-n + 15]:\n",
        "  hgg = [word_to_index[word] for word in line.split(' ')]\n",
        "  linelen = len(hgg)\n",
        "  for i in range(max_line - len(hgg)):\n",
        "    hgg.append(word_to_index[pad])\n",
        "  maskindex = torch.randint(linelen,[1]).item()\n",
        "  hgg[maskindex] = word_to_index['[MASK]']\n",
        "  print(' '.join([index_to_word[item] for item in hgg]))\n",
        "  hgg = torch.tensor(hgg).unsqueeze(0)\n",
        "  mask = (hgg != pad_index)\n",
        "  predict = model(hgg, mask)\n",
        "  values, indexes = torch.topk(predict[0][maskindex],3)\n",
        "  for item in indexes:\n",
        "    print('  ',index_to_word[item.item()])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLgsz4giQUvp",
        "outputId": "af3ff86e-564b-4319-d6fd-b3ef0b1e4f16"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm neutral [MASK] this [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
            "   about\n",
            "   on\n",
            "   this\n",
            "The food was [MASK] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
            "   satisfactory\n",
            "   delicious\n",
            "   outstanding\n",
            "The service at this hotel was [MASK] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
            "   great\n",
            "   [pad]\n",
            "   hotel\n",
            "This phone [MASK] standard [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
            "   is\n",
            "   restaurant\n",
            "   was\n",
            "The [MASK] is normal [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
            "   traffic\n",
            "   car\n",
            "   weather\n",
            "This movie was [MASK] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
            "   regular\n",
            "   ordinary\n",
            "   mediocre\n",
            "I have no strong opinion [MASK] this [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
            "   this\n",
            "   [pad]\n",
            "   on\n",
            "The [MASK] was unremarkable [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
            "   beach\n",
            "   book\n",
            "   museum\n",
            "[MASK] museum was standard [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
            "   The\n",
            "   I\n",
            "   I'm\n",
            "I have no strong feelings about [MASK] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
            "   this\n",
            "   [pad]\n",
            "   on\n",
            "The [MASK] was standard [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
            "   speaker\n",
            "   beach\n",
            "   game\n",
            "This software [MASK] standard [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
            "   is\n",
            "   restaurant\n",
            "   was\n",
            "The game [MASK] standard [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
            "   was\n",
            "   is\n",
            "   amenities\n",
            "The speaker was [MASK] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
            "   average\n",
            "   standard\n",
            "   boring\n",
            "I'm indifferent [MASK] this [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad] [pad]\n",
            "   with\n",
            "   this\n",
            "   fence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "that is amazing\n",
        "I masked one word in the sentence, then training to model to predict the masked word\n",
        "\n",
        "after some epoches, it works. even though, it maynot predicts the exact word, but it will predict very similar word\n",
        "\n",
        "will continue"
      ],
      "metadata": {
        "id": "y_9ByyHrMDU2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the same with encoder_sentiment\n",
        "updated with three things\n",
        "1: use a class to encapsulate the add and norm layer\n",
        "2: mask the pad tokens, so the pad tokens did not get attention\n",
        "3: add position encoding\n",
        "\n",
        "overall, the performance is better now."
      ],
      "metadata": {
        "id": "HZny9M16W43Q"
      }
    }
  ]
}